Evaluation Plan

At this point in the thesis, I would have already discussed the results of the combined effect on single-threaded performance of different combinations of Vertex-and-edge orderings. The single threaded “case-study” will act as a motivation for the next chapters in my thesis (already completed):

1.  Parallelization of Slashburn

2.  Parallelization of the edge-centric hilbert traversal via "Hilbert-Blocking"

Since I see this case-study as motivating the main contributions of my research, I don’t think I should include it in the evaluation. 

Tentatively, I see the evaluation chapter consisting of 2 main sections. 
Section 1: Ablation Study An ablation study of the cross-effects and interaction of:

1.  vertex ordering

2.  edge ordering

3.  matrix blocking

on the performance of multi-threaded edge-centric graph traversal (i.e. PageRank computation). This section will answer the following questions:

1.  I’ve shown that there is a performance benefit to be had by combining vertex-and-edge orderings in the single-threaded setting. How do these speedups translate to the multithreaded setting?

2.  For the more time-consuming reorderings, is it even worth it to perform these preprocessing steps?

I’ll address (b) by:

1.  For each step in the preprocessing pipeline, I would list the preprocessing time and the performance improvement (if any).

2.  If I see a speedup, I will also list the number of iterations it would take to amortize the cost of any preprocessing step.

It’s important to choose a proper baseline here. This could be the “original” vertex ID assignment of the input graph (some hyperlink networks that were constructed using crawlers have excellent “original” orderings - for these graphs, vertex reorderings have actually been shown to make performance worse). I could also choose a random vertex ordering, or both random and original orderings.

Section 2: Comparison of Runtime, Scaling of Hilbert Blocking and other STOA, in-memory Graph processing systems

This section will answer the following:

1.  How does Hilbert-Blocking compare to state-of-the-art graph processing systems?

    1.  I’ll measure runtime and L2, L3 cache miss ratios.

2.  I’ll also analyze the scaling behaviours of the different systems.

I’ll compare against Ligra, Galois, and GPOP. It’s important to note here that the different systems use different optimizations, whereas my “optimization” is more of a preprocessing pipeline:

1.  Vertex Reordering

2.  Edge Reordering and blocking using the Hilbert-Blocks

For this reason, I am not sure that it is an apples-to-apples comparison, but still I am able to show some promising results. I’ll use GPOP as an example for the optimizations these systems use. GPOP partitions the vertices into L2-sized, disjoint vertex sets. It then constructs a graph based on those partitions, with the partitions acting as vertices. If a vertex has multiple neighbours in another partition, GPOP will only send a single update to that partition - this reduces the number of updates written substantially. But, as Figure 1 shows, this system doesn’t scale well when other vertex orderings have been applied to the graph - specifically ones that make specific regions of the adjacency matrix denser.

[Runtime and Scaling for different vertex reorderings (Original, Descending (Out) Degree Sort, and Slashburn) for PageRank computation using GPOP and Hilbert Blocking.]

Takeaways from Figure 1

1.  For vertex reorderings that construct dense areas of the adjacency matrix (Slashburn, Degree sort), GPOP is unable to scale. This is to be expected, since a large proportion of the edges of the graph will be contained within a few partitions - those partitions that contain the high degree vertices of the graph, which are assigned a lower vertex ID value. Since partitions are processed on a per-core basis, the partitions containing the high-degree vertices will straggle, while the other cores will be blocked waiting for the others to complete.

2.  The baseline, single-core performance of Hilbert-blocking is consistently much worse than GPOP. Also to be expected, due to the system optimization discussed above.

3.  The combination of Hilbert blocking and Slashburn produces strong scaling behaviour. I need to experiment further to provide sufficient explanation to why this is.

4.  The best performing data point is using GPOP on the original twitter graph - 5.581s. Compared to the best performing data point using Hilbert Blocking and Slashburn - 6.387s. So, even without any preprocessing steps, GPOP outperforms the best Hilbert-Blocking time (which is especially telling since the Parallel Slashburn computation is lengthy - ≈ 20 minutes).

    -   I’m finding it difficult to construct a meaningful narrative based on these findings, but I think the following may make sense: I mention that vertex reorderings like Descending Degree Sort have been shown to be useful for other workloads (e.g. Triangle Counting). So, if we make the assumption that users would like to store their graph using such a vertex reordering, then these results advocate for the Hilbert Blocking technique.
